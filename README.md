<div align="center">

<img src="public/icons/logo.png" alt="FitMyCV Logo" width="360" />

# FitMyCV ‚Äî SaaS d'optimisation de CV par IA

[![En production](https://img.shields.io/badge/En_production-app.fitmycv.io-00C853)](https://app.fitmycv.io)
[![Next.js](https://img.shields.io/badge/Next.js-16-black?logo=next.js)](https://nextjs.org/)
[![React](https://img.shields.io/badge/React-19-61DAFB?logo=react&logoColor=white)](https://react.dev/)
[![JavaScript](https://img.shields.io/badge/JavaScript-ES2024-F7DF1E?logo=javascript&logoColor=black)](https://developer.mozilla.org/en-US/docs/Web/JavaScript)
[![PostgreSQL](https://img.shields.io/badge/PostgreSQL-Prisma_6-336791?logo=postgresql&logoColor=white)](https://www.postgresql.org/)
[![OpenAI](https://img.shields.io/badge/OpenAI-GPT--4.1_%7C_o4--mini_%7C_GPT--4o-412991?logo=openai&logoColor=white)](https://openai.com/)
[![Docker](https://img.shields.io/badge/Docker-Prod-2496ED?logo=docker&logoColor=white)](https://www.docker.com/)

**[app.fitmycv.io](https://app.fitmycv.io)** ¬∑ **[fitmycv.io](https://www.fitmycv.io)**

üá´üá∑ Fran√ßais | [üá¨üáß English](README.en.md)

</div>

---

## Pourquoi ce projet

J'ai construit FitMyCV comme un projet end-to-end pour d√©velopper et d√©montrer mes comp√©tences en **AI Engineering** dans un contexte de production r√©el ‚Äî pas un PoC, pas un notebook, mais un SaaS complet avec des utilisateurs, des paiements et une infrastructure de d√©ploiement.

Le produit adapte des CV √† des offres d'emploi via des pipelines LLM multi-√©tapes. La contrainte centrale : **l'IA ne fabrique rien** ‚Äî elle reformule et r√©organise uniquement le parcours existant du candidat, avec des garde-fous anti-hallucination appliqu√©s dans le code.

---

## Comp√©tences d√©montr√©es

### 1. Orchestration multi-mod√®les

Chaque phase du pipeline utilise un mod√®le choisi pour ses caract√©ristiques sp√©cifiques. Les affectations sont stock√©es en base de donn√©es et modifiables √† chaud.

| Phase | Mod√®le | Pourquoi ce mod√®le |
|-------|--------|-------------------|
| Extraction d'offre (HTML/Markdown bruit√©) | `o4-mini` | Raisonnement structur√© pour extraction complexe |
| Classification KEEP / REMOVE / MOVE | `gpt-4o` | Haute pr√©cision sur des d√©cisions discr√®tes |
| Adaptation des exp√©riences (√óN en parall√®le) | `o4-mini` (`reasoning_effort: low`) | R√©√©criture nuanc√©e, optimis√©e en vitesse |
| D√©duction des comp√©tences | `o4-mini` | Inf√©rence logique depuis le contenu adapt√© |
| Am√©lioration cibl√©e des exp√©riences | `gpt-4.1` | Qualit√© maximale pour les r√©√©critures critiques |
| Import PDF (multi-pages) | `gpt-4.1-mini` (Vision) | Extraction multi-modale page par page |
| Score de matching | `gpt-4.1-mini` | Scoring structur√© + suggestions |

**Ce que √ßa d√©montre** : la capacit√© √† s√©lectionner et configurer le bon mod√®le pour chaque t√¢che selon le compromis co√ªt / qualit√© / latence, plut√¥t que d'utiliser un mod√®le unique pour tout.

---

### 2. Prompt Engineering

Les prompts ne sont pas des cha√Ænes en dur ‚Äî c'est une architecture composable :

- **Fichiers Markdown** avec directive `{INCLUDE:chemin}` pour r√©utiliser des fragments communs (r√®gles d'adaptation, politique de langue, syst√®me de base)
- **Substitution de variables** (`{{experience}}`, `{{job_offer}}`) au moment de l'appel
- **Sch√©mas JSON Structured Output** versionn√©s aux c√¥t√©s de chaque prompt ‚Äî le LLM est contraint de respecter un format strict
- **Cache en production, hot reload en dev** ‚Äî le loader adapte son comportement selon l'environnement

```
lib/prompts-shared/              ‚Üê Fragments partag√©s
‚îú‚îÄ‚îÄ system-base.md
‚îú‚îÄ‚îÄ cv-adaptation-rules.md
‚îú‚îÄ‚îÄ scoring-rules.md
‚îî‚îÄ‚îÄ language-policy.md

lib/features/cv-adaptation/phases/batch-experiences/
‚îú‚îÄ‚îÄ system.md                    ‚Üê {INCLUDE:../../../prompts-shared/system-base.md}
‚îú‚îÄ‚îÄ user.md                      ‚Üê {{experience}}, {{job_offer}}, {{language}}
‚îî‚îÄ‚îÄ schemas/adaptationSchema.json
```

**Ce que √ßa d√©montre** : une ing√©nierie de prompts pens√©e pour la maintenabilit√© et la scalabilit√©, pas du copier-coller de cha√Ænes dans le code.

---

### 3. Pipeline IA de production

Deux pipelines distincts, chacun multi-√©tapes avec parall√©lisme contr√¥l√© :

**Pipeline Adaptation** (CV ‚Üí offre d'emploi) :

```
Offre (URL / PDF / Markdown)
  ‚Üí Phase 0    Extraction structur√©e (o4-mini, json_schema)
  ‚Üí Phase 0.5  Warmup cache (appel √† 50 tokens pour pr√©-charger le cache serveur OpenAI)
  ‚Üí Phase 1    Classification KEEP/REMOVE/MOVE (gpt-4o)
  ‚Üí Phase 2    7 t√¢ches en parall√®le (Promise.all) :
               exp√©riences ¬∑ projets ¬∑ comp√©tences ¬∑ r√©sum√© ¬∑ extras ¬∑ formation ¬∑ langues
  ‚Üí Phase 3    Recomposition (aucun appel LLM) + initialisation du mode r√©vision
```

**Pipeline Am√©lioration** (depuis les suggestions du score de matching) :

```
  ‚Üí Stage 1+2  Classification des comp√©tences + pr√©processeur (en parall√®le)
  ‚Üí Stage 3    Am√©lioration parall√®le par section (p-limit(5) appels concurrents)
  ‚Üí Stage 4    Mise √† jour du r√©sum√© post-am√©liorations
```

**D√©tails d'impl√©mentation notables** :
- La 1√®re exp√©rience s'ex√©cute seule, suivie d'un d√©lai de 500ms avant le batch restant ‚Äî pour laisser le cache serveur d'OpenAI se propager avant les appels parall√®les
- File de jobs in-process (`MAX_CONCURRENT_JOBS = 3`) avec limites par utilisateur, sans broker externe
- `AbortController` propag√© √† tous les appels OpenAI en cours ‚Äî l'annulation est instantan√©e et rembourse automatiquement les cr√©dits
- Progression pouss√©e en temps r√©el au client via Server-Sent Events (SSE)

**Ce que √ßa d√©montre** : la capacit√© √† orchestrer des appels LLM complexes en production avec gestion de la concurrence, des erreurs, de l'annulation et du feedback temps r√©el.

---

### 4. Garde-fous anti-hallucination

L'IA ne peut pas halluciner parce que le **code l'en emp√™che**, pas seulement le prompt :

| Garde-fou | Comment |
|-----------|---------|
| Champs immuables | `entreprise`, `dates`, `lieu` sont **supprim√©s** de l'objet avant l'appel LLM et **restaur√©s** depuis l'original apr√®s ‚Äî le mod√®le ne les voit jamais |
| R√©alisations quantifi√©es | Seuls les points contenant au moins un chiffre sont conserv√©s (`/\d/.test(item)`) |
| Score ind√©pendant | Le score global retourn√© par GPT est **ignor√©** ; seuls les sous-scores sont extraits et recombin√©s avec une formule pond√©r√©e fixe (35/30/20/15) |
| Structured Outputs | `json_schema` force le mod√®le √† respecter un format strict ‚Äî pas de texte libre |
| Mode r√©vision | Chaque modification IA est stock√©e dans `pendingChanges[]` ‚Äî l'utilisateur accepte ou refuse chaque diff |

**Ce que √ßa d√©montre** : une approche d√©fensive o√π la fiabilit√© repose sur l'architecture du code, pas sur la bonne volont√© du mod√®le.

---

### 5. Optimisation des co√ªts et t√©l√©m√©trie

Chaque appel OpenAI est trac√© individuellement et agr√©g√© quotidiennement :

- **Par appel** (`OpenAICall`) : tokens prompt / completion / cached, co√ªt, mod√®le, feature, dur√©e
- **Agr√©gat journalier** (`OpenAIUsage`) : userId √ó feature √ó mod√®le, avec √©conomies de cache
- **R√©f√©rentiel de prix** (`OpenAIPricing`) : tarifs standard + priority tier par mod√®le
- **Alertes sur seuils** (`OpenAIAlert`) : d√©pense quotidienne / mensuelle, par utilisateur ou globale
- **Strat√©gie de cache** : warmup de prompt, d√©lai inter-batch pour propagation, `reasoning_effort: low` sur les t√¢ches batch

**Ce que √ßa d√©montre** : la ma√Ætrise des co√ªts et la capacit√© √† instrumenter un syst√®me IA pour le suivre et l'optimiser en continu.

---

### 6. D√©veloppement Full-Stack SaaS

Au-del√† de l'IA, le projet couvre l'ensemble du spectre SaaS :

| Domaine | Impl√©mentation |
|---------|---------------|
| **Frontend** | Next.js 16 (App Router), React 19, Tailwind CSS 4 ‚Äî 138 composants |
| **Backend** | 113 routes API, Prisma 6, 34 mod√®les de donn√©es |
| **Auth** | NextAuth.js ‚Äî email/password, Google, GitHub, Apple OAuth |
| **Paiements** | Stripe ‚Äî cr√©dits √† la carte, sans abonnement |
| **Extension navigateur** | Chrome / Firefox Manifest V3 ‚Äî extraction d'offres depuis 11 sites d'emploi |
| **Import / Export** | Vision API (PDF‚ÜíJSON), export PDF et DOCX avec templates |
| **Multi-langues** | Fran√ßais, Anglais, Allemand, Espagnol |
| **Infrastructure** | Docker, GitHub Actions (CI/CD automatis√©), Caddy, Cloudflare |
| **Versioning** | Historique complet des CV, restauration en un clic |

---

## Stack technique

| Couche | Technologies |
|--------|-------------|
| **Application** | Next.js 16, React 19, Tailwind CSS 4, JavaScript ES2024 |
| **Base de donn√©es** | PostgreSQL, Prisma 6 (34 mod√®les) |
| **IA** | OpenAI API ‚Äî GPT-4.1, GPT-4o, o4-mini, GPT-4.1-mini (Vision) |
| **Auth** | NextAuth.js (Google, GitHub, Apple) |
| **Paiements** | Stripe |
| **Extension** | Vite, Manifest V3, Readability, Turndown |
| **Infra** | Docker, GitHub Actions, Caddy, Cloudflare |

---

## Documentation

La documentation technique compl√®te est disponible dans [`docs/`](./docs/) :

| Document | Contenu |
|----------|---------|
| [Architecture](./docs/architecture.md) | Architecture syst√®me |
| [API Reference](./docs/api-reference.md) | 113 endpoints |
| [Data Models](./docs/data-models.md) | 34 mod√®les Prisma |
| [Components](./docs/components.md) | 138 composants React |

---

## Licence

Propri√©taire ‚Äî Tous droits r√©serv√©s.
